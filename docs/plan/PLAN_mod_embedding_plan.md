### 1. [Gap Analysis] "추상화 레벨 불일치" 문제 보완이 필요함

가장 우려되는 부분은 데이터의 성격 차이

- **부처 R&R (Query):** "산업통상자원부는 상업, 무역, 공업... 사무를 관장한다." (매우 추상적/포괄적)
- **법안 (Document):** "제3조(설비 기준) 반도체 제조 시설의..." (매우 구체적/세부적)

임베딩 모델은 이 둘의 **추상화 레벨이 다를 때 유사도를 낮게 평가**하는 경향이 있음 

**[보완안] R&R 데이터 증강 (Augmentation)**

- 대안 소스 ①: 소관 법령 목록 (Laws under Jurisdiction)
    - **논리:** 산업통상자원부가 관리하는 법률들의 **'이름'** 자체가 가장 강력하고 구체적인 키워드입니다.
    - **데이터:**
        - *산업부 소관 법률:* "반도체 집적회로의 배치설계에 관한 법률", "산업집적활성화 및 공장설립에 관한 법률", "송유관 안전관리법"...
    - **적용:** R&R 텍스트 뒤에 이 **법률명 리스트**를 붙입니다. 법안 텍스트에 "공장 설립"이나 "송유관"이 나오면 바로 매칭됩니다.
- 대안 소스 ②: 하부 조직명 (Organization Chart)
    - **논리:** 부처의 '국'이나 '과' 이름에 실무 키워드가 다 들어있습니다.
    - **데이터:**
        - *산업부 직제:* "반도체디스플레이과", "원전산업정책과", "재생에너지보급과", "섬유탄소나노과"...
    - **적용:** 직제 규정(대통령령)에서 국/과 이름만 긁어와서 R&R에 추가합니다.

## 클로드 야 

R&R 데이터 증강 (Fact 기반):
   - 각 부처의 [소관 법령 목록]과 [하부 조직명(실/국/과)]을 R&R 텍스트에 추가하는 전처리를 수행해줘.
   - 예: 산업부 R&R += "반도체법, 공장설립법, 전력수급기본계획, 원전산업정책과, 섬유탄소나노과..."
   - 이렇게 하면 추상적인 R&R 문장과 구체적인 법안 텍스트 사이의 간극을 '법적 용어'로 메울 수 있어.

### 2. 🎯 [Evaluation] "Golden Set(정답셋)" 수동 구축

계획서의 `4.1 정량 지표` 중 **"Cross-Domain 감지율"**은 정답(Ground Truth)이 없으면 측정이 불가능함. 100개 법안 중 무엇이 '타 부처 리스크'인지 기계는 알 수 없음. 

**[보완안] 5개의 'Golden Sample' 지정**

- 100개를 다 검증할 시간은 없음
- 샘플링한 100개 중, **"이건 무조건 산업부가 봐야 해"** 싶은 법안(예: 환경노동위의 탄소중립 법안 등)을 사람이 직접 마킹 (내가)
- **평가 방법:** 모델들이 이 5개 법안을 산업부 R&R과 유사하다고 판단하는지(Top-Ranked) 확인
- **지표:** `Recall@Top10` (상위 10개 안에 내 정답 5개가 들어왔는가?)

## 클로드 야

Golden Set 평가 (유지):
   - 100개 샘플 중 타겟 부처(예: 산업부)와 연관성이 확실한 5개를 수동으로 마킹하고, 모델이 이걸 찾아내는지 Recall 측정.

### 3. ⚙️ [Technical] 모델별 "입력 한계(Context Length)" 대응

법안의 `제안이유`나 `주요내용`은 텍스트가 긴 편임. 모델마다 받아들일 수 있는 길이가 다름

- **Solar Embedding:** 4096 토큰 (넉넉함)
- **KoSimCSE:** 512 토큰 (짧음) → **잘림(Truncation) 발생**
- **text-embedding-3-small:** 8191 토큰 (매우 넉넉함)

KoSimCSE 처럼 짧은 모델 테스트시 법안의 서론만, 아니면 중요키워드 요약 등으로 대응 하려 하였으나 오 히려 이런 Truncation은 실험 조건을 해칠 수 있다는 판단임. 
입력 길이 제한이 512토큰인 모델들(KoSimCSE, multilingual-e5)은 "긴 법률 문서 처리에 부적합"하므로 이번 비교군에서 제외하자.

대신 실험 대상을 Long Context(4096+)를 지원하는 3개 모델로 집중해서 "공정한 조건"에서 비교해줘.

[수정된 비교 대상]
1. Solar Embedding (4k, 한국어 특화 API)
2. text-embedding-3-small (8k, 가성비 API)
3. BGE-M3 (8k, 로컬/오픈소스, Dense+Sparse)

이렇게 3개만 남기면, 데이터 전처리(Truncation) 고민 없이 "법률 뉘앙스를 누가 더 잘 파악하나"에만 집중해서 평가할 수 있을 거야.